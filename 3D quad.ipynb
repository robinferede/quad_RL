{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import stable_baselines3\n",
    "import sys\n",
    "\n",
    "print(\"python version:\", sys.version)\n",
    "print(\"stable_baselines3 version:\", stable_baselines3.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda version:\", torch.version.cuda)\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set torch default device\n",
    "torch.set_default_device(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation of Motion 3D Quadcopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload sympy\n",
    "from sympy import *\n",
    "\n",
    "# Equations of motion 3D quadcopter from https://arxiv.org/pdf/2304.13460.pdf\n",
    "\n",
    "# w1,w2,w3,w4 are the motor speeds normalized to [-1,1]\n",
    "# u1,u2,u3,u4 are the motor commands normalized to [-1,1]\n",
    "\n",
    "state = symbols('x y z v_x v_y v_z phi theta psi p q r w1 w2 w3 w4')\n",
    "x,y,z,vx,vy,vz,phi,theta,psi,p,q,r,w1,w2,w3,w4 = state\n",
    "control = symbols('u_1 u_2 u_3 u_4')\n",
    "u1,u2,u3,u4 = control\n",
    "\n",
    "# parameters\n",
    "g = 9.81\n",
    "Ixx = 0.000906\n",
    "Iyy = 0.001242\n",
    "Izz = 0.002054\n",
    "\n",
    "k_x  = 1.07933887e-05\n",
    "k_y  = 9.65250793e-06\n",
    "k_z  = 2.7862899e-05\n",
    "k_w  = 4.36301076e-08\n",
    "k_h  = 0.0625501332\n",
    "k_p  = 1.4119331e-09\n",
    "k_pv = -0.00797101848\n",
    "k_q  = 1.21601884e-09\n",
    "k_qv = 0.0129263739\n",
    "k_r1 = 2.57035545e-06\n",
    "k_r2 = 4.10923364e-07\n",
    "k_rr = 0.000812932607\n",
    "\n",
    "tau = 0.06\n",
    "w_min = 3000\n",
    "w_max = 12000\n",
    "\n",
    "\n",
    "# Rotation matrix \n",
    "Rx = Matrix([[1, 0, 0], [0, cos(phi), -sin(phi)], [0, sin(phi), cos(phi)]])\n",
    "Ry = Matrix([[cos(theta), 0, sin(theta)], [0, 1, 0], [-sin(theta), 0, cos(theta)]])\n",
    "Rz = Matrix([[cos(psi), -sin(psi), 0], [sin(psi), cos(psi), 0], [0, 0, 1]])\n",
    "R = Rz*Ry*Rx\n",
    "\n",
    "# Body velocity\n",
    "vbx, vby, vbz = R.T@Matrix([vx,vy,vz])\n",
    "\n",
    "# normalized motor speeds to rpm\n",
    "W1 = (w1+1)/2*(w_max-w_min) + w_min\n",
    "W2 = (w2+1)/2*(w_max-w_min) + w_min\n",
    "W3 = (w3+1)/2*(w_max-w_min) + w_min\n",
    "W4 = (w4+1)/2*(w_max-w_min) + w_min\n",
    "\n",
    "# first order delay\n",
    "d_w1 = (u1 - w1)/tau\n",
    "d_w2 = (u2 - w2)/tau\n",
    "d_w3 = (u3 - w3)/tau\n",
    "d_w4 = (u4 - w4)/tau\n",
    "\n",
    "# derivative of rpm: d/dt[((w+1)/2*(w_max-w_min) + w_min)]\n",
    "d_W1 = d_w1/2*(w_max-w_min)\n",
    "d_W2 = d_w2/2*(w_max-w_min)\n",
    "d_W3 = d_w3/2*(w_max-w_min)\n",
    "d_W4 = d_w4/2*(w_max-w_min)\n",
    "\n",
    "# Thrust and Drag\n",
    "T = -k_w*(W1**2 + W2**2 + W3**2 + W4**2) - k_h*(vbx**2+vby**2) - k_z*vbz*(W1+W2+W3+W4)\n",
    "Dx = -k_x*vbx*(W1+W2+W3+W4)\n",
    "Dy = -k_y*vby*(W1+W2+W3+W4)\n",
    "\n",
    "# Moments\n",
    "Mx = k_p*(W1**2-W2**2-W3**2+W4**2) + k_pv*vy\n",
    "My = k_q*(W1**2+W2**2-W3**2-W4**2) + k_qv*vx\n",
    "Mz = k_r1*(-W1+W2-W3+W4) + k_r2*(-d_W1+d_W2-d_W3+d_W4) - k_rr*r\n",
    "\n",
    "# Dynamics\n",
    "d_x = vx\n",
    "d_y = vy\n",
    "d_z = vz\n",
    "\n",
    "d_vx, d_vy, d_vz = Matrix([0,0,g]) + R@Matrix([Dx, Dy,T])\n",
    "\n",
    "d_phi   = p + q*sin(phi)*tan(theta) + r*cos(phi)*tan(theta)\n",
    "d_theta = q*cos(phi) - r*sin(phi)\n",
    "d_psi   = q*sin(phi)/cos(theta) + r*cos(phi)/cos(theta)\n",
    "\n",
    "d_p     = (q*r*(Iyy-Izz) + Mx)/Ixx\n",
    "d_q     = (p*r*(Izz-Ixx) + My)/Iyy\n",
    "d_r     = (p*q*(Ixx-Iyy) + Mz)/Izz\n",
    "\n",
    "# State space model\n",
    "f = [d_x, d_y, d_z, d_vx, d_vy, d_vz, d_phi, d_theta, d_psi, d_p, d_q, d_r, d_w1, d_w2, d_w3, d_w4]\n",
    "\n",
    "# lambdify\n",
    "f_func = lambdify((Array(state), Array(control)), Array(f), 'numpy')\n",
    "# f_jax = lambdify((Array(state), Array(control)), Array(f), 'jax')\n",
    "# f_num = lambdify((Array(state), Array(control)), Array(f), 'numexpr')\n",
    "# f_cup = lambdify((Array(state), Array(control)), Array(f), 'cupy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gym environment for the 3d quadcopter model\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class Quadcopter3D(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Define the action space and observation space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,))\n",
    "        # all states are  in [-inf, inf] except for the last 4 states which are in [w_min, w_max]\n",
    "        self.observation_space = spaces.Box(low=np.array([-np.inf]*12+[-1]*4), high=np.array([-np.inf]*12+[1]*4), dtype=np.float32)\n",
    "        \n",
    "        # Define any other environment-specific parameters\n",
    "        self.max_steps = 500  # Maximum number of steps in an episode\n",
    "        self.dt = 0.01  # Time step duration\n",
    "        self.goal_threshold = 0.1  # Threshold for considering the goal reached\n",
    "\n",
    "        # Define any other necessary variables\n",
    "        self.state = np.zeros(16)\n",
    "        self.step_count = 0\n",
    "        self.action = (0, 0, 0, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        x0 =  np.random.uniform(-5,5)\n",
    "        y0 =  np.random.uniform(-5,5)\n",
    "        z0 =  np.random.uniform(-5,5)\n",
    "\n",
    "        vx0    = np.random.uniform(-1,1)\n",
    "        vy0    = np.random.uniform(-1,1)\n",
    "        vz0    = np.random.uniform(-1,1)\n",
    "\n",
    "        phi0   = np.random.uniform(-1,1)\n",
    "        theta0 = np.random.uniform(-1,1)\n",
    "        psi0   = np.random.uniform(-np.pi,np.pi)\n",
    "\n",
    "        p0     = np.random.uniform(-1,1)\n",
    "        q0     = np.random.uniform(-1,1)\n",
    "        r0     = np.random.uniform(-1,1)\n",
    "\n",
    "        w10    = np.random.uniform(-1,1)\n",
    "        w20    = np.random.uniform(-1,1)\n",
    "        w30    = np.random.uniform(-1,1)\n",
    "        w40    = np.random.uniform(-1,1)\n",
    "\n",
    "        self.state = np.array([x0, y0, z0, vx0, vy0, vz0, phi0, theta0, psi0, p0, q0, r0, w10, w20, w30, w40])\n",
    "        self.step_count = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Simulate the environment one step forward in time with the given action\n",
    "        self.step_count += 1        \n",
    "        self.action = action\n",
    "        self.state = self.state + self.dt*f_func(self.state, self.action)\n",
    "\n",
    "        # Compute the reward\n",
    "        x,y,z,vx,vy,vz,phi,theta,psi,p,q,r,w1,w2,w3,w4 = self.state\n",
    "        #reward = 1-(x**2+y**2+z**2)/300\n",
    "\n",
    "        # Rewards from https://github.com/uzh-rpg/flightmare/blob/competition/flightpy/configs/control/config.yaml\n",
    "        pos_coeff = -0.02\n",
    "        vel_coeff = -0.001\n",
    "        ang_coeff = -0.02\n",
    "        ang_vel_coeff = -0.001\n",
    "\n",
    "        pos_reward = pos_coeff * np.linalg.norm(self.state[0:3])\n",
    "        vel_reward = vel_coeff * np.linalg.norm(self.state[3:6])\n",
    "        ang_reward = ang_coeff * np.linalg.norm(self.state[6:9])\n",
    "        ang_vel_reward = ang_vel_coeff * np.linalg.norm(self.state[9:12])\n",
    "\n",
    "        reward = 1+pos_reward + vel_reward + ang_reward + ang_vel_reward\n",
    "\n",
    "        # Check if the goal has been reached\n",
    "        goal_reached = np.linalg.norm(self.state[0:12]) < self.goal_threshold\n",
    "        if goal_reached:\n",
    "            reward = 100\n",
    "\n",
    "        # Check if out of bounds\n",
    "        out_of_bounds = np.any(np.abs(self.state[0:3]) > 10) or np.any(np.abs(self.state[6:9]) > np.pi)\n",
    "\n",
    "        if out_of_bounds:\n",
    "            reward = -1\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = goal_reached or out_of_bounds or self.step_count >= self.max_steps\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        # Outputs a dict containing all information for rendering\n",
    "        state_dict = dict(zip(['x','y','z','vx','vy','vz','phi','theta','psi','p','q','r','w1','w2','w3','w4'], self.state))\n",
    "        # Rescale actions to [0,1] for rendering\n",
    "        action_dict = dict(zip(['u1','u2','u3','u4'], (np.array(self.action)+1)/2))\n",
    "        return {**state_dict, **action_dict}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Vector Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient vectorized version of the environment\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "class Quadcopter3DVec(VecEnv):\n",
    "    def __init__(self, num_envs):\n",
    "        # action space and observation space (with extra dimension for the number of environments)\n",
    "        action_space = spaces.Box(low=-1, high=1, shape=(4,))\n",
    "        observation_space = spaces.Box(low=np.array([-np.inf]*12+[-1]*4), high=np.array([-np.inf]*12+[1]*4))\n",
    "        VecEnv.__init__(self,num_envs, observation_space, action_space)\n",
    "\n",
    "        # Define any other environment-specific parameters\n",
    "        self.max_steps = 1000  # Maximum number of steps in an episode\n",
    "        self.dt = 0.01  # Time step duration\n",
    "\n",
    "        # Define the hover state thresholds\n",
    "        self.pos_threshold = 0.3  # max 30cm error\n",
    "        self.vel_threshold = 0.3  # max 30cm/s error\n",
    "        self.ang_threshold = 10*np.pi/180  # max 10 degree error\n",
    "        self.rat_threshold = 10*np.pi/180  # max 10 degree/s error\n",
    "\n",
    "        # Define any other necessary variables\n",
    "        self.states = np.zeros((num_envs,16))\n",
    "        self.step_counts = np.zeros(num_envs)\n",
    "        self.actions = np.zeros((num_envs,4))\n",
    "\n",
    "    def reset_(self, dones):\n",
    "        num_reset = dones.sum()\n",
    "        # Reset the state of the environment to an initial state\n",
    "        x0 =  np.random.uniform(-5,5, size=(num_reset,))\n",
    "        y0 =  np.random.uniform(-5,5, size=(num_reset,))\n",
    "        z0 =  np.random.uniform(-5,5, size=(num_reset,))\n",
    "\n",
    "        vx0    = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        vy0    = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        vz0    = np.random.uniform(-1,1, size=(num_reset,))\n",
    "\n",
    "        phi0    = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        theta0  = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        psi0    = np.random.uniform(-np.pi, np.pi, size=(num_reset,))\n",
    "\n",
    "        p0      = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        q0      = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        r0      = np.random.uniform(-1,1, size=(num_reset,))\n",
    "\n",
    "        w10     = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w20     = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w30     = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w40     = np.random.uniform(-1,1, size=(num_reset,))\n",
    "\n",
    "        self.states[dones] = np.stack([x0, y0, z0, vx0, vy0, vz0, phi0, theta0, psi0, p0, q0, r0, w10, w20, w30, w40], axis=1)\n",
    "        self.step_counts[dones] = np.zeros(num_reset)\n",
    "        return self.states\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.reset_(np.ones(self.num_envs, dtype=bool))\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        self.actions = actions\n",
    "    \n",
    "    def step_wait(self):\n",
    "        # Simulate the environment one step forward in time with the given action\n",
    "        self.step_counts += 1\n",
    "        self.states = self.states + self.dt*f_func(self.states.T, self.actions.T).T\n",
    "\n",
    "        # Rewards from https://github.com/uzh-rpg/flightmare/blob/competition/flightpy/configs/control/config.yaml\n",
    "        pos_reward = -0.002*np.linalg.norm(self.states[:,0:3], axis=1)\n",
    "        vel_reward = -0.002*np.linalg.norm(self.states[:,3:6], axis=1)\n",
    "        ang_reward = -0.0001*np.linalg.norm(self.states[:,6:9], axis=1)\n",
    "        rat_reward = -0.0001*np.linalg.norm(self.states[:,9:12], axis=1)\n",
    "\n",
    "        rewards = pos_reward + vel_reward + ang_reward + rat_reward\n",
    "        \n",
    "        # Check if the goal has been reached\n",
    "        pos_reached = np.linalg.norm(self.states[:,0:3], axis=1) < self.pos_threshold\n",
    "        vel_reached = np.linalg.norm(self.states[:,3:6], axis=1) < self.vel_threshold\n",
    "        ang_reached = np.all(np.abs(self.states[:,6:9]) < self.ang_threshold, axis=1)\n",
    "        rat_reached = np.all(np.abs(self.states[:,9:12]) < self.rat_threshold, axis=1)\n",
    "\n",
    "        goal_reached = pos_reached & vel_reached & ang_reached & rat_reached\n",
    "        rewards[goal_reached] = 100\n",
    "\n",
    "        # Check if out of bounds\n",
    "        out_of_bounds = out_of_bounds = np.any(np.abs(self.states[:,0:3]) > 10, axis=1) | np.any(np.abs(self.states[:,6:8]) > np.pi, axis=1)\n",
    "        rewards[out_of_bounds] = -1\n",
    "\n",
    "        # Check number of steps\n",
    "        max_steps_reached = self.step_counts >= self.max_steps\n",
    "\n",
    "        # Check if the episode is done\n",
    "        dones = goal_reached | out_of_bounds | max_steps_reached\n",
    "\n",
    "        if np.any(max_steps_reached):\n",
    "            print('max_steps_reached')\n",
    "        if np.any(out_of_bounds):\n",
    "            print('out_of_bounds')\n",
    "        if np.any(goal_reached):\n",
    "            print('goal_reached')\n",
    "\n",
    "        # reset env if done\n",
    "        self.reset_(dones)\n",
    "\n",
    "        # Write info dicts\n",
    "        infos = [{}] * self.num_envs\n",
    "        for i in range(self.num_envs):\n",
    "            if dones[i]:\n",
    "                infos[i][\"terminal_observation\"] = self.states[i]\n",
    "            if max_steps_reached[i] or out_of_bounds[i]:\n",
    "                infos[i][\"TimeLimit.truncated\"] = True\n",
    "        return self.states, rewards, dones, infos\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    def get_attr(self, attr_name, indices=None):\n",
    "        pass\n",
    "\n",
    "    def set_attr(self, attr_name, value, indices=None):\n",
    "        pass\n",
    "\n",
    "    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n",
    "        pass\n",
    "\n",
    "    def env_is_wrapped(self, wrapper_class, indices=None):\n",
    "        return [False]*self.num_envs\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Outputs a dict containing all information for rendering\n",
    "        state_dict = dict(zip(['x','y','z','vx','vy','vz','phi','theta','psi','p','q','r','w1','w2','w3','w4'], self.states.T))\n",
    "        # Rescale actions to [0,1] for rendering\n",
    "        action_dict = dict(zip(['u1','u2','u3','u4'], (np.array(self.actions.T)+1)/2))\n",
    "        return {**state_dict, **action_dict}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test animation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quadcopter_animation import animation\n",
    "# reload modules\n",
    "import importlib\n",
    "importlib.reload(animation)\n",
    "import time\n",
    "\n",
    "num = 100\n",
    "env = Quadcopter3DVec(num_envs=num)\n",
    "\n",
    "# Run a random agent\n",
    "env.reset()\n",
    "# print(env.render())\n",
    "done = False\n",
    "def run():\n",
    "    global done\n",
    "    action = np.random.uniform(-1,1, size=(num,4))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    # wait a bit\n",
    "    # print(env.step_counts[0])\n",
    "    return env.render()\n",
    "\n",
    "animation.view(run)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Train the PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "models_dir = 'models/ppo_3DquadVec'\n",
    "log_dir = 'logs/ppo_3DquadVec'\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Date and time string for unique folder names\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create SubprocVecEnv for parallelization\n",
    "# env = make_vec_env(lambda: Quadcopter3D(), n_envs=20)\n",
    "\n",
    "# Create the environment\n",
    "env = Quadcopter3DVec(num_envs=100)\n",
    "\n",
    "# Wrap the environment in a Monitor wrapper\n",
    "env = VecMonitor(env)\n",
    "\n",
    "# ReLU net with 3 hidden layers of size 120\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch=[dict(pi=[120,120,120], vf=[120,120,120])],\n",
    "    log_std_init = 0\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    "    tensorboard_log=log_dir,\n",
    "    n_steps=500,\n",
    "    batch_size=5000,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "# add tanh layer to action net\n",
    "from torch import nn\n",
    "model.policy.action_net.add_module('tanh', nn.Tanh())\n",
    "print(model.policy)\n",
    "\n",
    "def train():\n",
    "    TIMESTEPS = 10000\n",
    "    for i in range(1,10000000000):\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name='PPO_'+datetime_str)\n",
    "        model.save(models_dir + '/PPO_' + datetime_str + '/' + str(i))\n",
    "        print('iteration: ', i, '  timestep: ', i*TIMESTEPS)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quadcopter_animation import animation\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import numpy as np\n",
    "import importlib\n",
    "importlib.reload(animation)\n",
    "\n",
    "def animate_runs(model, env, **kwargs):\n",
    "    env.reset()\n",
    "    def run():\n",
    "        actions, _ = model.predict(env.states)\n",
    "        states, rewards, dones, infos = env.step(actions)\n",
    "        return env.render()\n",
    "    animation.view(run, **kwargs)\n",
    "\n",
    "# make function that loads latest model from model folder\n",
    "def load_latest_model(model_dir, index=-1):\n",
    "    # get all folders in model_dir\n",
    "    folders = os.listdir(model_dir)\n",
    "    # get all folders that start with PPO_\n",
    "    folders = [folder for folder in folders if folder.startswith('PPO_')]\n",
    "    # get the latest folder\n",
    "    latest_folder = sorted(folders)[index]\n",
    "    print('latest_folder=',latest_folder)\n",
    "    # get all files in the latest folder\n",
    "    files = os.listdir(model_dir + '/' + latest_folder)\n",
    "    # get the file with the highest number as name\n",
    "    latest_file = max(files, key=lambda x: int(x[:-4]))\n",
    "    print('latest_file=',latest_file)\n",
    "    # load the latest model\n",
    "    model = PPO.load(model_dir + '/' + latest_folder + '/' + latest_file)\n",
    "    return model\n",
    "    \n",
    "models_dir = 'models/ppo_3DquadVec'\n",
    "model = load_latest_model(models_dir)\n",
    "animate_runs(model, Quadcopter3DVec(1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flying through gates!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient vectorized version of the environment\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "class Quadcopter3DVecGates(VecEnv):\n",
    "    def __init__(self, num_envs, gates_pos, gate_yaw, start_pos):\n",
    "        # action space and observation space (with extra dimension for the number of environments)\n",
    "        action_space = spaces.Box(low=-1, high=1, shape=(4,))\n",
    "        observation_space = spaces.Box(low=np.array([-np.inf]*12+[-1]*4), high=np.array([-np.inf]*12+[1]*4))\n",
    "        VecEnv.__init__(self,num_envs, observation_space, action_space)\n",
    "\n",
    "        # Define any other environment-specific parameters\n",
    "        self.max_steps = 1000  # Maximum number of steps in an episode\n",
    "        self.dt = 0.01  # Time step duration\n",
    "\n",
    "        # Define the race track\n",
    "        self.start_pos = start_pos.astype(np.float32)\n",
    "        self.gate_pos = gates_pos.astype(np.float32)\n",
    "        self.gate_yaw = gate_yaw.astype(np.float32)\n",
    "        self.num_gates = gates_pos.shape[0]\n",
    "        self.target_gates = np.zeros(num_envs, dtype=int)\n",
    "\n",
    "        # Define any other necessary variables\n",
    "        self.states = np.zeros((num_envs,16), dtype=np.float32)\n",
    "        self.step_counts = np.zeros(num_envs, dtype=np.float32)\n",
    "        self.actions = np.zeros((num_envs,4), dtype=np.float32)\n",
    "        \n",
    "\n",
    "    def reset_(self, dones):\n",
    "        num_reset = dones.sum()\n",
    "\n",
    "        # Sample random starting positions between the gates\n",
    "        points = np.concatenate([self.start_pos[np.newaxis,:], self.gate_pos], axis=0)\n",
    "        segment = np.random.randint(0, self.num_gates, size=(num_reset,))\n",
    "\n",
    "        # Compute the starting position\n",
    "        start_pos = (points[segment]+points[segment+1])/2\n",
    "        # start_pos = self.start_pos[np.newaxis, :]\n",
    "\n",
    "        x0 = 0.1*np.random.randn(num_reset) + start_pos[:,0]\n",
    "        y0 = 0.1*np.random.randn(num_reset) + start_pos[:,1]\n",
    "        z0 = 0.1*np.random.randn(num_reset) + start_pos[:,2]\n",
    "        \n",
    "        vx0 = 0.1*np.random.randn(num_reset)\n",
    "        vy0 = 0.1*np.random.randn(num_reset)\n",
    "        vz0 = 0.1*np.random.randn(num_reset)\n",
    "        \n",
    "        phi0   = 1*np.random.randn(num_reset)\n",
    "        theta0 = 1*np.random.randn(num_reset)\n",
    "        psi0   = 1*np.random.randn(num_reset)\n",
    "        \n",
    "        p0 = 0.1*np.random.randn(num_reset)\n",
    "        q0 = 0.1*np.random.randn(num_reset)\n",
    "        r0 = 0.1*np.random.randn(num_reset)\n",
    "        \n",
    "        w10 = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w20 = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w30 = np.random.uniform(-1,1, size=(num_reset,))\n",
    "        w40 = np.random.uniform(-1,1, size=(num_reset,))\n",
    "\n",
    "        self.states[dones] = np.stack([x0, y0, z0, vx0, vy0, vz0, phi0, theta0, psi0, p0, q0, r0, w10, w20, w30, w40], axis=1)\n",
    "        self.step_counts[dones] = np.zeros(num_reset)\n",
    "        self.target_gates[dones] = segment #np.zeros(num_reset, dtype=int)\n",
    "        return self.states\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.reset_(np.ones(self.num_envs, dtype=bool))\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        self.actions = actions\n",
    "    \n",
    "    def step_wait(self):\n",
    "        # Simulate the environment one step forward in time with the given action\n",
    "        self.step_counts += 1\n",
    "        new_states = self.states + self.dt*f_func(self.states.T, self.actions.T).T\n",
    "\n",
    "        pos_old = self.states[:,0:3]\n",
    "        pos_new = new_states[:,0:3]\n",
    "        pos_gate = self.gate_pos[self.target_gates]\n",
    "        \n",
    "        # Rewards from [secret paper]\n",
    "        d2g_old = np.linalg.norm(pos_old - pos_gate, axis=1)\n",
    "        d2g_new = np.linalg.norm(pos_new - pos_gate, axis=1)\n",
    "        rat_penalty = 0.0001*np.linalg.norm(new_states[:,9:12], axis=1)\n",
    "        rewards = d2g_old - d2g_new - rat_penalty\n",
    "        \n",
    "        # Check if the gate has been passed\n",
    "        normal = np.array([np.cos(self.gate_yaw[self.target_gates]), np.sin(self.gate_yaw[self.target_gates])]).T\n",
    "        # dot product of normal and position vector over axis 1\n",
    "        pos_old_projected = (pos_old[:,0]-pos_gate[:,0])*normal[:,0] + (pos_old[:,1]-pos_gate[:,1])*normal[:,1]\n",
    "        pos_new_projected = (pos_new[:,0]-pos_gate[:,0])*normal[:,0] + (pos_new[:,1]-pos_gate[:,1])*normal[:,1]\n",
    "        passed_gate_plane = (pos_old_projected < 0) & (pos_new_projected > 0)\n",
    "        gate_passed = passed_gate_plane & np.all(np.abs(pos_new - pos_gate)<0.5, axis=1)\n",
    "        #rewards[gate_passed] = 10\n",
    "        \n",
    "        # Check for gate collision\n",
    "        gate_collision = passed_gate_plane & np.any(np.abs(pos_new - pos_gate)>0.5, axis=1)\n",
    "        rewards[gate_collision] = -10\n",
    "\n",
    "        # Check ground collision (z > 0)\n",
    "        ground_collision = self.states[:,2] > 0\n",
    "        rewards[ground_collision] = -10\n",
    "        \n",
    "        # Check out of bounds\n",
    "        # outside grid abs(x,y)>10\n",
    "        # prevent numerical issues: abs(p,q,r) < 1000\n",
    "        out_of_bounds = np.any(np.abs(self.states[:,0:2]) > 10, axis=1) | np.any(np.abs(self.states[:,9:12]) > 1000, axis=1)\n",
    "        \n",
    "        # Check number of steps\n",
    "        max_steps_reached = self.step_counts >= self.max_steps\n",
    "\n",
    "        # Update target gate\n",
    "        self.target_gates[gate_passed] += 1\n",
    "        \n",
    "        # Check if final gate has been passed\n",
    "        final_gate_passed = self.target_gates >= self.num_gates\n",
    "\n",
    "        # give reward for passing final gate\n",
    "        rewards[final_gate_passed] = 10\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        dones = max_steps_reached | gate_collision | ground_collision | final_gate_passed | out_of_bounds\n",
    "        \n",
    "        # Update states\n",
    "        self.states = new_states\n",
    "        \n",
    "        # reset env if done\n",
    "        self.reset_(dones)\n",
    "\n",
    "        if np.any(gate_collision):\n",
    "            print('gate_collision', gate_collision.sum())\n",
    "\n",
    "        # Write info dicts\n",
    "        infos = [{}] * self.num_envs\n",
    "        for i in range(self.num_envs):\n",
    "            if dones[i]:\n",
    "                infos[i][\"terminal_observation\"] = self.states[i]\n",
    "            if max_steps_reached[i]:\n",
    "                infos[i][\"TimeLimit.truncated\"] = True\n",
    "        return self.states, rewards, dones, infos\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    def get_attr(self, attr_name, indices=None):\n",
    "        pass\n",
    "\n",
    "    def set_attr(self, attr_name, value, indices=None):\n",
    "        pass\n",
    "\n",
    "    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n",
    "        pass\n",
    "\n",
    "    def env_is_wrapped(self, wrapper_class, indices=None):\n",
    "        return [False]*self.num_envs\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Outputs a dict containing all information for rendering\n",
    "        state_dict = dict(zip(['x','y','z','vx','vy','vz','phi','theta','psi','p','q','r','w1','w2','w3','w4'], self.states.T))\n",
    "        # Rescale actions to [0,1] for rendering\n",
    "        action_dict = dict(zip(['u1','u2','u3','u4'], (np.array(self.actions.T)+1)/2))\n",
    "        return {**state_dict, **action_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Race Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(animation)\n",
    "\n",
    "# Define the race track\n",
    "gate_pos = np.array([\n",
    "    [-1.5,-2,-1.5],\n",
    "    [1.5,2,-1.5],\n",
    "    [1.5,-2,-1.5],\n",
    "    [-1.5,2,-1.5]\n",
    "]*2)\n",
    "\n",
    "gate_yaw = np.array([\n",
    "    0,\n",
    "    0,\n",
    "    np.pi,\n",
    "    np.pi,\n",
    "]*2)\n",
    "\n",
    "start_pos = np.array([-4,-2,-1.5])\n",
    "\n",
    "# gate_pos = gate_pos[0:2]\n",
    "# gate_yaw = gate_yaw[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Animation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quadcopter_animation import animation\n",
    "# reload modules\n",
    "import importlib\n",
    "importlib.reload(animation)\n",
    "import time\n",
    "\n",
    "num = 10\n",
    "env = Quadcopter3DVecGates(num_envs=num, gates_pos=gate_pos, gate_yaw=gate_yaw, start_pos=start_pos)\n",
    "\n",
    "# Run a random agent\n",
    "env.reset()\n",
    "# print(env.render())\n",
    "done = False\n",
    "def run():\n",
    "    global done\n",
    "    action = np.random.uniform(-1,1, size=(num,4))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    # wait a bit\n",
    "    # print(env.step_counts[0])\n",
    "    return env.render()\n",
    "\n",
    "animation.view(run, gate_pos=gate_pos, gate_yaw=gate_yaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "models_dir = 'models/ppo_3DquadVecGates'\n",
    "log_dir = 'logs/ppo_3DquadVecGates'\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Date and time string for unique folder names\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create the environment\n",
    "env = Quadcopter3DVecGates(num_envs=100, gates_pos=gate_pos, gate_yaw=gate_yaw, start_pos=start_pos)\n",
    "\n",
    "# Wrap the environment in a Monitor wrapper\n",
    "env = VecMonitor(env)\n",
    "\n",
    "# ReLU net with 3 hidden layers of size 120\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch=[dict(pi=[120,120,120], vf=[120,120,120])],\n",
    "    log_std_init = 0\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    "    tensorboard_log=log_dir,\n",
    "    n_steps=1000,\n",
    "    batch_size=5000,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "# add tanh layer to action net\n",
    "from torch import nn\n",
    "model.policy.action_net.add_module('tanh', nn.Tanh())\n",
    "print(model.policy)\n",
    "\n",
    "def train(model, train_env, test_env):\n",
    "    TIMESTEPS = model.n_steps*env.num_envs*10\n",
    "    for i in range(1,10000000000):\n",
    "        model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name='PPO_'+datetime_str)\n",
    "        model.save(models_dir + '/PPO_' + datetime_str + '/' + str(i))\n",
    "        # animate_progress(model, test_env)\n",
    "\n",
    "def animate_progress(model, env):\n",
    "    env.reset()\n",
    "    def run():\n",
    "        actions, _ = model.predict(env.states)\n",
    "        states, rewards, dones, infos = env.step(actions)\n",
    "        return env.render()\n",
    "    animation.view(run, gate_pos=gate_pos, gate_yaw=gate_yaw)\n",
    "\n",
    "# run training loop\n",
    "test_env = Quadcopter3DVecGates(num_envs=10, gates_pos=gate_pos, gate_yaw=gate_yaw, start_pos=start_pos)\n",
    "train(model, env, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(animation)\n",
    "\n",
    "def animate_progress(models_dir, env, **kwargs):\n",
    "    model = load_latest_model(models_dir, index=-1)\n",
    "    env.reset()\n",
    "    def run():\n",
    "        actions, _ = model.predict(env.states)\n",
    "        states, rewards, dones, infos = env.step(actions)\n",
    "        # print('rewards', rewards)\n",
    "        return env.render()\n",
    "    animation.view(run, **kwargs)\n",
    "\n",
    "test_env = Quadcopter3DVecGates(num_envs=1, gates_pos=gate_pos, gate_yaw=gate_yaw, start_pos=start_pos)\n",
    "models_dir = 'models/ppo_3DquadVecGates'\n",
    "animate_progress(models_dir, test_env, gate_pos=gate_pos, gate_yaw=gate_yaw)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill training thread\n",
    "5.77/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
